{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-10 19:16:53.399095: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /Users/rlo/Documents/00Proyectos/00_CPG_Generic_object_detector/fiftyone_VIT_model_zoo\n",
      "Model exist locally, not downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at ./ and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "MODEL_GITHUB_URL = \"https://github.com/Pervasive-Technologies/pt-cpg-google-vit-large-patch16-224/\"  # Replace with your actual URL\n",
    "\n",
    "foz.register_zoo_model_source(MODEL_GITHUB_URL)\n",
    "\n",
    "# Load dataset\n",
    "#dataset = fo.load_dataset(\"your-dataset\")\n",
    "\n",
    "# Load the model\n",
    "model = foz.load_zoo_model(\"pt-cpg/google/vit-large-patch16-224\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://github.com/Pervasive-Technologies/pt-cpg-google-vit-large-patch16-224/']\n"
     ]
    }
   ],
   "source": [
    "remote_sources = foz.list_zoo_model_sources()\n",
    "\n",
    "print(remote_sources)\n",
    "# [..., \"https://github.com/voxel51/openai-clip\", ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet-imagenet-torch', 'centernet-hg104-1024-coco-tf2', 'centernet-hg104-512-coco-tf2', 'centernet-mobilenet-v2-fpn-512-coco-tf2', 'centernet-resnet101-v1-fpn-512-coco-tf2', 'centernet-resnet50-v1-fpn-512-coco-tf2', 'centernet-resnet50-v2-512-coco-tf2', 'classification-transformer-torch', 'clip-vit-base32-torch', 'deeplabv3-cityscapes-tf', 'deeplabv3-mnv2-cityscapes-tf', 'deeplabv3-resnet101-coco-torch', 'deeplabv3-resnet50-coco-torch', 'densenet121-imagenet-torch', 'densenet161-imagenet-torch', 'densenet169-imagenet-torch', 'densenet201-imagenet-torch', 'depth-estimation-transformer-torch', 'detection-transformer-torch', 'dinov2-vitb14-torch', 'dinov2-vitg14-torch', 'dinov2-vitl14-torch', 'dinov2-vits14-torch', 'efficientdet-d0-512-coco-tf2', 'efficientdet-d0-coco-tf1', 'efficientdet-d1-640-coco-tf2', 'efficientdet-d1-coco-tf1', 'efficientdet-d2-768-coco-tf2', 'efficientdet-d2-coco-tf1', 'efficientdet-d3-896-coco-tf2', 'efficientdet-d3-coco-tf1', 'efficientdet-d4-1024-coco-tf2', 'efficientdet-d4-coco-tf1', 'efficientdet-d5-1280-coco-tf2', 'efficientdet-d5-coco-tf1', 'efficientdet-d6-1280-coco-tf2', 'efficientdet-d6-coco-tf1', 'efficientdet-d7-1536-coco-tf2', 'faster-rcnn-inception-resnet-atrous-v2-coco-tf', 'faster-rcnn-inception-resnet-atrous-v2-lowproposals-coco-tf', 'faster-rcnn-inception-v2-coco-tf', 'faster-rcnn-nas-coco-tf', 'faster-rcnn-nas-lowproposals-coco-tf', 'faster-rcnn-resnet101-coco-tf', 'faster-rcnn-resnet101-lowproposals-coco-tf', 'faster-rcnn-resnet50-coco-tf', 'faster-rcnn-resnet50-fpn-coco-torch', 'faster-rcnn-resnet50-lowproposals-coco-tf', 'fcn-resnet101-coco-torch', 'fcn-resnet50-coco-torch', 'googlenet-imagenet-torch', 'inception-resnet-v2-imagenet-tf1', 'inception-v3-imagenet-torch', 'inception-v4-imagenet-tf1', 'keypoint-rcnn-resnet50-fpn-coco-torch', 'mask-rcnn-inception-resnet-v2-atrous-coco-tf', 'mask-rcnn-inception-v2-coco-tf', 'mask-rcnn-resnet101-atrous-coco-tf', 'mask-rcnn-resnet50-atrous-coco-tf', 'mask-rcnn-resnet50-fpn-coco-torch', 'med-sam-2-video-torch', 'mnasnet0.5-imagenet-torch', 'mnasnet1.0-imagenet-torch', 'mobilenet-v2-imagenet-tf1', 'mobilenet-v2-imagenet-torch', 'open-clip-torch', 'pt-cpg/google/vit-large-patch16-224', 'resnet-v1-50-imagenet-tf1', 'resnet-v2-50-imagenet-tf1', 'resnet101-imagenet-torch', 'resnet152-imagenet-torch', 'resnet18-imagenet-torch', 'resnet34-imagenet-torch', 'resnet50-imagenet-torch', 'resnext101-32x8d-imagenet-torch', 'resnext50-32x4d-imagenet-torch', 'retinanet-resnet50-fpn-coco-torch', 'rfcn-resnet101-coco-tf', 'rtdetr-l-coco-torch', 'rtdetr-x-coco-torch', 'segment-anything-2-hiera-base-plus-image-torch', 'segment-anything-2-hiera-base-plus-video-torch', 'segment-anything-2-hiera-large-image-torch', 'segment-anything-2-hiera-large-video-torch', 'segment-anything-2-hiera-small-image-torch', 'segment-anything-2-hiera-small-video-torch', 'segment-anything-2-hiera-tiny-image-torch', 'segment-anything-2-hiera-tiny-video-torch', 'segment-anything-2.1-hiera-base-plus-image-torch', 'segment-anything-2.1-hiera-base-plus-video-torch', 'segment-anything-2.1-hiera-large-image-torch', 'segment-anything-2.1-hiera-large-video-torch', 'segment-anything-2.1-hiera-small-image-torch', 'segment-anything-2.1-hiera-small-video-torch', 'segment-anything-2.1-hiera-tiny-image-torch', 'segment-anything-2.1-hiera-tiny-video-torch', 'segment-anything-vitb-torch', 'segment-anything-vith-torch', 'segment-anything-vitl-torch', 'segmentation-transformer-torch', 'shufflenetv2-0.5x-imagenet-torch', 'shufflenetv2-1.0x-imagenet-torch', 'squeezenet-1.1-imagenet-torch', 'squeezenet-imagenet-torch', 'ssd-inception-v2-coco-tf', 'ssd-mobilenet-v1-coco-tf', 'ssd-mobilenet-v1-fpn-640-coco17', 'ssd-mobilenet-v1-fpn-coco-tf', 'ssd-mobilenet-v2-320-coco17', 'ssd-resnet50-fpn-coco-tf', 'vgg11-bn-imagenet-torch', 'vgg11-imagenet-torch', 'vgg13-bn-imagenet-torch', 'vgg13-imagenet-torch', 'vgg16-bn-imagenet-torch', 'vgg16-imagenet-tf1', 'vgg16-imagenet-torch', 'vgg19-bn-imagenet-torch', 'vgg19-imagenet-torch', 'wide-resnet101-2-imagenet-torch', 'wide-resnet50-2-imagenet-torch', 'yolo-nas-torch', 'yolo-v2-coco-tf1', 'yolo11l-coco-torch', 'yolo11l-seg-coco-torch', 'yolo11m-coco-torch', 'yolo11m-seg-coco-torch', 'yolo11n-coco-torch', 'yolo11n-seg-coco-torch', 'yolo11s-coco-torch', 'yolo11s-seg-coco-torch', 'yolo11x-coco-torch', 'yolo11x-seg-coco-torch', 'yolov10l-coco-torch', 'yolov10m-coco-torch', 'yolov10n-coco-torch', 'yolov10s-coco-torch', 'yolov10x-coco-torch', 'yolov5l-coco-torch', 'yolov5m-coco-torch', 'yolov5n-coco-torch', 'yolov5s-coco-torch', 'yolov5x-coco-torch', 'yolov8l-coco-torch', 'yolov8l-obb-dotav1-torch', 'yolov8l-oiv7-torch', 'yolov8l-seg-coco-torch', 'yolov8l-world-torch', 'yolov8m-coco-torch', 'yolov8m-obb-dotav1-torch', 'yolov8m-oiv7-torch', 'yolov8m-seg-coco-torch', 'yolov8m-world-torch', 'yolov8n-coco-torch', 'yolov8n-obb-dotav1-torch', 'yolov8n-oiv7-torch', 'yolov8n-seg-coco-torch', 'yolov8s-coco-torch', 'yolov8s-obb-dotav1-torch', 'yolov8s-oiv7-torch', 'yolov8s-seg-coco-torch', 'yolov8s-world-torch', 'yolov8x-coco-torch', 'yolov8x-obb-dotav1-torch', 'yolov8x-oiv7-torch', 'yolov8x-seg-coco-torch', 'yolov8x-world-torch', 'yolov9c-coco-torch', 'yolov9c-seg-coco-torch', 'yolov9e-coco-torch', 'yolov9e-seg-coco-torch', 'zero-shot-classification-transformer-torch', 'zero-shot-detection-transformer-torch']\n"
     ]
    }
   ],
   "source": [
    "available_models = foz.list_zoo_models()\n",
    "\n",
    "print(available_models)\n",
    "# [..., \"voxel51/clip-vit-base32-torch\", ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset exists and delete it\n",
    "if \"my_custom_vit_dataset\" in fo.list_datasets():\n",
    "    fo.delete_dataset(\"my_custom_vit_dataset\")\n",
    "    print(\"Deleted dataset: my_custom_vit_dataset\")\n",
    "else:\n",
    "    print(\"Dataset 'my_custom_vit_dataset' does not exist.\")\n",
    "\n",
    "# Reload the dataset (if it wasn't deleted)\n",
    "if \"my_custom_vit_dataset\" in fo.list_datasets():\n",
    "    dataset = fo.load_dataset(\"my_custom_vit_dataset\")\n",
    "    \n",
    "    # Check if similarity index exists\n",
    "    if \"custom_vit_similarity\" in dataset.list_brain_runs():\n",
    "        dataset.delete_brain_run(\"custom_vit_similarity\")\n",
    "        print(\"Deleted brain key: custom_vit_similarity\")\n",
    "    else:\n",
    "        print(\"No similarity index named 'custom_vit_similarity' found.\")\n",
    "else:\n",
    "    print(\"No dataset found, so no brain key deletion needed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create a FiftyOne dataset\n",
    "dataset = fo.load_dataset(\"my_custom_vit_dataset\") if \"my_custom_vit_dataset\" in fo.list_datasets() else fo.Dataset(\"my_custom_vit_dataset\")\n",
    "dataset.persistent = True\n",
    "\n",
    "# Load images into the dataset\n",
    "dataset.add_samples([\n",
    "    fo.Sample(filepath=\"./test/image1.jpg\"),\n",
    "    fo.Sample(filepath=\"./test/image2.jpg\"),\n",
    "    fo.Sample(filepath=\"./test/image3.jpg\"),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "fob.compute_similarity(\n",
    "    dataset,\n",
    "    model= model,\n",
    "    brain_key=\"custom_vit_similarity\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Custom ViT similarity index computed!\")\n",
    "\n",
    "# Save the dataset\n",
    "dataset.save()\n",
    "\n",
    "# Print information about the created FiftyOne dataset\n",
    "print(dataset)\n",
    "\n",
    "# To visualize the dataset in the FiftyOne App (optional)\n",
    "session = fo.launch_app(dataset)\n",
    "\n",
    "# Blocks execution until the App is closed\n",
    "session.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
